{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionClassifier.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "06FpRNYQllir",
        "colab_type": "code",
        "outputId": "5bc5208f-b0e3-4841-da3f-d953bd1b3192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1cj0lFlI0TOm",
        "colab_type": "code",
        "outputId": "2b685517-c208-40d5-ece6-b23b5f65cfd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "! unzip gdrive/My\\ Drive/fer2013.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  gdrive/My Drive/fer2013.zip\n",
            "replace fer2013.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4K0iS-x_1MXX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IM8zH31D2M27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 7 # angry, disgust, fear, happy, sad, surprise, neutral\n",
        "batch_size = 256\n",
        "epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GKiUPe_piyZb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with open(\"fer2013.csv\") as f:\n",
        "#   content = f.readlines()\n",
        " \n",
        "# lines = np.array(content)\n",
        " \n",
        "# num_of_instances = lines.size\n",
        "# print(\"number of instances: \",num_of_instances)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lLsBiAIM8ZM0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for i in range(1, num_of_instances):\n",
        "  \n",
        "#   try:\n",
        "#     emotion, img, usage = lines[i].split(\",\")\n",
        "    \n",
        "#     val = img.split(\" \")\n",
        "#     pixels = np.array(val, 'float32')\n",
        "    \n",
        "#     if 'Training' in usage:\n",
        "#       y_train.append(emotion)\n",
        "#       x_train.append(pixels)\n",
        "      \n",
        "#     elif 'PublicTest' in usage:\n",
        "#       y_test.append(emotion)\n",
        "#       x_test.append(pixels)\n",
        "      \n",
        "#   except:\n",
        "#     print(\"\", end=\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOqj4ADD9MWg",
        "colab_type": "code",
        "outputId": "2eea8962-4884-4af4-fdec-d733c3f85559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('fer2013.csv', header=0)\n",
        "data.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['emotion', 'pixels', 'Usage'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "jqBrA8E49Z_V",
        "colab_type": "code",
        "outputId": "c4fcdb48-2e04-4a34-b099-a081cc77a530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "grouped = data.groupby('Usage')\n",
        "\n",
        "train = grouped.get_group('Training')\n",
        "test = grouped.get_group('PrivateTest')\n",
        "validation = grouped.get_group('PublicTest')\n",
        "print(len(train), len(test), len(validation))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28709 3589 3589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A1UuqcDnC2_3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(train)\n",
        "df.drop(['Usage'], axis=1, inplace=True)\n",
        "df.to_csv(\"train.csv\", index=False)\n",
        "\n",
        "# df = pd.DataFrame(y_train)\n",
        "# df.to_csv(\"y_train.csv\", index=False)\n",
        "\n",
        "df = pd.DataFrame(test)\n",
        "df.drop(['Usage'], axis=1, inplace=True)\n",
        "df.to_csv(\"test.csv\", index=False)\n",
        "\n",
        "# df = pd.DataFrame(y_test)\n",
        "# df.to_csv(\"y_test.csv\", index=False)\n",
        "\n",
        "df = pd.DataFrame(validation)\n",
        "df.drop(['Usage'], axis=1, inplace=True)\n",
        "df.to_csv(\"validation.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OoyrboTS21IT",
        "colab_type": "code",
        "outputId": "a639ac59-b60b-4c38-cdee-b5fc825cc1c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "cell_type": "code",
      "source": [
        "! pip install torch==0.4.0 torchvision"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 30kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "\u001b[31mfastai 1.0.51 has requirement torch>=1.0.0, but you'll have torch 0.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.0.1.post2\n",
            "    Uninstalling torch-1.0.1.post2:\n",
            "      Successfully uninstalled torch-1.0.1.post2\n",
            "Successfully installed torch-0.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "R85pSFChoxj8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data as Data\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLazR0ArAxfg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FerDataset(Data.Dataset):\n",
        "  \n",
        "  def __init__(self, csv_path, transforms=None):\n",
        "    \n",
        "    \n",
        "    self.transform = transforms\n",
        "    \n",
        "    self.df = pd.read_csv(csv_path)\n",
        "    \n",
        "    #self.split = split\n",
        "    \n",
        "#     self.train = train\n",
        "    \n",
        "#     self.valid = valid\n",
        "    \n",
        "    #self.data = pd.read_csv(csv_path)\n",
        "    \n",
        "    \n",
        "#     with open(csv_path) as f:\n",
        "#       content = f.readlines()\n",
        " \n",
        "#     lines = np.array(content)\n",
        "  \n",
        "#     num_of_instances = lines.size\n",
        "#     #print(\"number of instances: \",num_of_instances)\n",
        "    \n",
        "#     self.train_data, self.train_labels, self.Test_data, self.Test_labels, self.valid_data, self.valid_labels = [], [], [], [], [], []\n",
        "    \n",
        "#     for i in range(1, num_of_instances):\n",
        "  \n",
        "#       try:\n",
        "#         emotion, img = lines[i].split(\",\")\n",
        "        \n",
        "#         val = img.split(\" \")\n",
        "#         pixels = np.array(val, 'int64')\n",
        "      \n",
        "#         if self.train == True:\n",
        "#           self.train_labels.append(emotion)\n",
        "#           self.train_data.append(pixels)\n",
        "          \n",
        "#         elif self.valid == True:\n",
        "#           self.valid_labels.append(emotion)\n",
        "#           self.valid_data.append(pixels)\n",
        "      \n",
        "#         else:\n",
        "#           self.Test_labels.append(emotion)\n",
        "#           self.Test_data.append(pixels)\n",
        "      \n",
        "#       except:\n",
        "#         print(\"\", end=\"\")\n",
        "    \n",
        "    \n",
        "#     if self.train == True:\n",
        "      \n",
        "# #       self.train_data = self.data['pixels']\n",
        "# #       self.train_labels = self.data['emotion']\n",
        "#        self.train_data = np.asarray(self.train_data)\n",
        "# #       print(type(self.train_data))\n",
        "# #       print(type(self.train_data[1]))\n",
        "#        self.train_data = self.train_data.reshape((28709, 48, 48))\n",
        "# #       print(self.train_data[1])\n",
        "     \n",
        "    \n",
        "#     elif self.valid == True: \n",
        "      \n",
        "# #       self.Test_data = self.data['pixels']\n",
        "# #       self.Test_labels = self.data['emotion']\n",
        "#        self.valid_data = np.asarray(self.valid_data)\n",
        "#        self.valid_data = self.valid_data.reshape((3589, 48, 48))\n",
        "    \n",
        "#     else: \n",
        "      \n",
        "# #       self.Test_data = self.data['pixels']\n",
        "# #       self.Test_labels = self.data['emotion']\n",
        "#        self.Test_data = np.asarray(self.Test_data)\n",
        "#        self.Test_data = self.Test_data.reshape((3589, 48, 48))\n",
        "      \n",
        "      \n",
        "  def __len__(self):\n",
        "    \n",
        "    return len(self.df)\n",
        "#     if self.train == True:\n",
        "#       return len(self.train_data)\n",
        "    \n",
        "#     elif self.valid == True:\n",
        "#       return len(self.valid_data)\n",
        "    \n",
        "#     else:\n",
        "#       return len(self.Test_data)\n",
        "    \n",
        "  \n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    image = self.df.iloc[idx, 1]\n",
        "    image = np.array(image.split(\" \"), 'int64').reshape((1, 48, 48))\n",
        "    target = self.df.iloc[idx, 0]\n",
        "    \n",
        "#     if self.train == True:\n",
        "      \n",
        "#       img, target = self.train_data[idx], self.train_labels[idx]\n",
        "      \n",
        "#     elif self.valid == True:\n",
        "      \n",
        "#       img, target = self.valid_data[idx], self.valid_labels[idx]\n",
        "\n",
        "#     else:\n",
        "      \n",
        "#       img, target = self.Test_data[idx], self.Test_labels[idx]\n",
        "      \n",
        "#     img = img[:, :, np.newaxis]\n",
        "#     img = np.concatenate((img, img, img), axis=2)\n",
        "#    img = Image.fromarray(image)\n",
        "    \n",
        "    if self.transform is not None:\n",
        "      \n",
        "       img = self.transform(image)\n",
        "      \n",
        "#     sample = {'image': img, 'target': target}\n",
        "\n",
        "\n",
        "    \n",
        "    return img, target\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ycotIVR-0hi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(44),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.TenCrop(44),\n",
        "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ooT8COpCHHmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainset = FerDataset('train.csv', transforms=transform_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tICJ32gZKZJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GpJ9CPATDmIp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainloader = Data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2KVkZyzKUu4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testset = FerDataset('test.csv', transforms=transform_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGj5GD2ZNNqr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testloader = Data.DataLoader(testset, batch_size=256, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IZLRfXnrnOIQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "validset = FerDataset('validation.csv', transforms=transform_train)\n",
        "validloader = Data.DataLoader(validset, batch_size=256, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fh3SA91xLMeH",
        "colab_type": "code",
        "outputId": "d2e1e555-478d-4c35-8e7c-85b7521f5295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "cell_type": "code",
      "source": [
        "train_iter = iter(trainloader)\n",
        "print(type(train_iter))\n",
        "#img, label = trainset.__getitem__(0)\n",
        "#print(img.shape)\n",
        "#print(type(img))\n",
        "print(type(trainset))\n",
        "images, labels = train_iter.next()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.utils.data.dataloader._DataLoaderIter'>\n",
            "<class '__main__.FerDataset'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-8a3f8732d30f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(type(img))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-37-a84b706d6c4f>\", line 117, in __getitem__\n    img = self.transform(image)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 60, in __call__\n    img = t(img)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 470, in __call__\n    i, j, h, w = self.get_params(img, self.size)\n  File \"/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\", line 443, in get_params\n    w, h = img.size\nTypeError: 'int' object is not iterable\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "o2KBOt106Zqa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e4d65be-fa96-4773-de4b-ad1fa9290029"
      },
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NQhfvGbQNaFd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pc0CQNSzmZZu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class model(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    \n",
        "    super(model, self).__init__()\n",
        "    \n",
        "    n_features = 48*48\n",
        "    \n",
        "    n_out = 7\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1)\n",
        "    \n",
        "    self.conv2 = nn.Conv2d(in_channels=16 , out_channels=32, kernel_size=3, stride=1)\n",
        "    \n",
        "    self.conv3 = nn.Conv2d(in_channels=32 , out_channels=64, kernel_size=3, stride=1)\n",
        "    \n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "    self.fc = nn.Linear(n_features, 64)\n",
        "    \n",
        "    self.out = nn.Linear(64, n_out)\n",
        "    \n",
        "    self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    \n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = F.relu(self.conv1(x))\n",
        "    \n",
        "    x = self.pool(x)\n",
        "    \n",
        "    x = F.relu(self.conv2(x))\n",
        "    \n",
        "    x = self.pool(x)\n",
        "    \n",
        "    x = F.relu(self.conv3(x))\n",
        "    \n",
        "    x = self.pool(x)\n",
        "    \n",
        "    x = x.view(-1, 48*48)\n",
        "    \n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    x = F.relu(self.fc(x))\n",
        "    \n",
        "    x = self.dropout(x)\n",
        "    \n",
        "    x = self.out(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WCez4VcamKBc",
        "colab_type": "code",
        "outputId": "d9d93f4c-20da-439a-bce9-0fc6f832d1a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "cell_type": "code",
      "source": [
        "net = model()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model(\n",
            "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=2304, out_features=64, bias=True)\n",
            "  (out): Linear(in_features=64, out_features=7, bias=True)\n",
            "  (dropout): Dropout(p=0.2)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3f3uXWXbgiHO",
        "colab_type": "code",
        "outputId": "95216aa4-c24d-4b64-c83b-ff869d511885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print('Training on CPU...')\n",
        "  \n",
        "else:\n",
        "  print('Training on GPU...')\n",
        "    \n",
        "if train_on_gpu:\n",
        "  net.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "58-x-ceahNEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.03)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UVwkelychhkV",
        "colab_type": "code",
        "outputId": "6f1514c0-36e7-4961-d303-4c396936092f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        }
      },
      "cell_type": "code",
      "source": [
        "# Training the network\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  \n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "  \n",
        "  net.train()\n",
        "  \n",
        "  for i in range(len(trainset)):\n",
        "    \n",
        "    sample = trainset[i]\n",
        "    img = sample['img']\n",
        "    target = sample['target']\n",
        "    \n",
        "    if train_on_gpu:\n",
        "      img, target = img.cuda(), target.cuda()\n",
        "      \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = net(img)\n",
        "    \n",
        "    loss = criterion(output, target)\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    train_loss += loss.item()*img.size(0)\n",
        "    \n",
        "    \n",
        "    \n",
        "  net.eval()\n",
        "\n",
        "  for i in range(len(validset)):\n",
        "    \n",
        "    sample = validset[i]\n",
        "    img = sample['img']\n",
        "    target = sample['target']\n",
        "  \n",
        "    if train_on_gpu:\n",
        "        img, target = img.cuda(), target.cuda()\n",
        "      \n",
        "    output = net(img)\n",
        "    \n",
        "    loss = criterion(output, target)\n",
        "  \n",
        "    valid_loss += loss.item()*img.size(0)\n",
        "  \n",
        "  \n",
        "  train_loss = train_loss/len(trainloader.dataset)\n",
        "  valid_loss = valid_loss/len(validloader.dataset)\n",
        "\n",
        "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "\n",
        "  if valid_loss <= valid_loss_min:\n",
        "          print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "          valid_loss_min,\n",
        "          valid_loss))\n",
        "          torch.save(net.state_dict(), 'net_fer2013.pt')\n",
        "          valid_loss_min = valid_loss\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-2248eb03ac1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-48767ae11885>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(img, output_size)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \"\"\"\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0mth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtw\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "8_E8hfJsi7Hs",
        "colab_type": "code",
        "outputId": "55a73d75-b0d6-4ddc-d515-6746a04d6cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "type(trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.FerDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "QbONXt51D_qJ",
        "colab_type": "code",
        "outputId": "435f21d3-fa32-4322-d827-2f3f838551a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(trainset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<__main__.FerDataset object at 0x7fb937a03f60>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bRW4yamVE9yZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}